{
    "model_type": "mlp",
    "input_shape": [128],
    "layer_specs": [
        {"type": "linear", "in_features": 128, "out_features": 64},
        {"type": "activation", "name": "relu"},
        {"type": "dropout", "dropout": 0.2},
        {"type": "linear", "in_features": 64, "out_features": 32},
        {"type": "activation", "name": "relu"},
        {"type": "linear", "in_features": 32, "out_features": 10}
    ],
    "device": "cuda"
}